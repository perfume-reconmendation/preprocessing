{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def tokenize(doc):\n",
    "    return tokenizer.tokenize(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To Small letter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "def to_small_letter(word: str):\n",
    "    return word.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 특수문자 및 외국어 제외\n",
    "\n",
    "* 알파벳, 그리고 의미가 담겨있을 가능성이 있는 `, ', . 를 제외하고 전부 제외한다.\n",
    "* 알파벳과 비슷하게 생겼지만 알파벳이 아닌 문자도 삭제된다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# function\n",
    "\n",
    "import re\n",
    "pattern = re.compile(\"[^a-z.`']\")\n",
    "def except_non_english(pattern, word):\n",
    "    return pattern.sub('', word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trimming (양 끝의 구두점과 공백 제거)\n",
    "\n",
    "* Ph.D 등 가운데에 있는 점은 의미가 있는 경우가 많다. 따라서 끝의 점만 삭제한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def trim(word: str):\n",
    "    return word.strip('.').strip(' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 불용어 제거"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(words: [str], custom_stopwords: set):\n",
    "    stop_words = set(stopwords.words('english')) | custom_stopwords\n",
    "    return [w for w in words if w not in stop_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## spelling 교정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correction(word):\n",
    "    return spell.correction(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 품사 태깅 + Lemmatization\n",
    "\n",
    "* 품사 태깅과 Lemmatization 은 떼어놓을 수 없는 관계\n",
    "* 품사를 기반으로 lemmatize 하기 때문\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('P'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "n = WordNetLemmatizer()\n",
    "def lemmatize_with_pos(words):\n",
    "    words_with_pos = nltk.pos_tag(words)\n",
    "\n",
    "    lemmatized = []\n",
    "    for w, pos in words_with_pos:\n",
    "        pos = get_wordnet_pos(pos)\n",
    "        if pos != '':\n",
    "            lemmatized.append(n.lemmatize(w, pos))\n",
    "        else:\n",
    "            lemmatized.append(w)\n",
    "\n",
    "    return lemmatized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "0     [get, sample, today, year, old, daughter, thin...\n1     [first, impression, test, blotter, hear, alien...\n2     [perfume, reminds, best, friend, actually, per...\n3     [imagine, trip, foot, fall, face, first, jasmi...\n4     [gorgeous, gorgeous, blend, love, scent, fan, ...\n5     [test, recent, formulation, ala, shell, former...\n6     [last, six, month, i've, hear, good, bad, revi...\n7     [honestly, not, smelt, say, safe, blind, buy, ...\n8     [i've, recently, discover, perfume, find, new,...\n9     [vote, love, love, meant, old, version, new, f...\n10    [surprised, much, enjoy, fragrance, much, talk...\n11    [pretty, synthetic, smell, scent, ive, find, s...\n12    [smell, different, expect, little, light, less...\n13    [lightly, sweet, jasmine, spicy, warm, amber, ...\n14    [wow, adamant, try, one, use, eat, year, back,...\n15                               [alien, banger, thats]\n16    [first, full, bottle, purchase, get, perfumes,...\n17    [hate, men, fragrance, mean, say, stay, away, ...\n18    [definitely, dont, get, hype, one, love, jasmi...\n19    [one, jasmine, bomb, complete, elegant, simpli...\nName: review, dtype: object"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map(f, iter):\n",
    "    return [f(e) for e in iter]\n",
    "\n",
    "import re\n",
    "pattern = re.compile(\"[^a-z.`']\")\n",
    "\n",
    "custom_stopwords = {'example'}\n",
    "\n",
    "def preprocessing(doc):\n",
    "    words = tokenize(doc)\n",
    "    words = map(to_small_letter, words)\n",
    "    words = [except_non_english(pattern, w) for w in words]\n",
    "    words = map(trim, words)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    words = remove_stopwords(words, custom_stopwords)\n",
    "    words = map(correction, words)\n",
    "    words = lemmatize_with_pos(words)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    return words\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../top_90_by_gender.csv')\n",
    "\n",
    "df['review'][:20].map(lambda x: preprocessing(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "def preprocess_df(df: pd.DataFrame, col_name, stopwords={' '}):\n",
    "    s = df['tokenized'] = df[col_name].map(lambda doc: map(to_small_letter, tokenize(doc)))\n",
    "    s = df['only_english'] = s.map(lambda words: map(trim, [except_non_english(pattern, w) for w in words]))\n",
    "    s = df['longer_than_2_A'] = s.map(lambda words: [w for w in words if len(w) > 2])\n",
    "    s = df['stopwords_removed'] = s.map(lambda words: remove_stopwords(words, stopwords))\n",
    "    # s = df['orthography'] = s.map(lambda words: map(correction, words))\n",
    "    s = df['lemmatizated'] = s.map(lambda words: lemmatize_with_pos(words))\\\n",
    "        .map(lambda words: [w for w in words if len(w) > 2])\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "['sur',\n 'libre',\n 'muse',\n 'serge',\n 'eau',\n 'maison',\n 'hermes',\n 'greatness',\n 'angels',\n 'molecule',\n 'poison',\n 'gio',\n 'cedar',\n 'sauvage',\n 'coco',\n 'lab',\n 'crystal',\n 'alien',\n 'mugger',\n '2011',\n 'martin',\n 'moschino',\n 'good',\n 'memoirs',\n 'est',\n 'one',\n 'kurkdjian',\n 'marly',\n 'allure',\n 'laurent',\n \"l'interdit\",\n 'chloe',\n 'fahrenheit',\n 'herrera',\n 'arma',\n 'versace',\n 'tom',\n 'guiltier',\n 'montage',\n 'baccarat',\n 'dune',\n 'givenchy',\n 'nuit',\n 'into',\n 'boise',\n 'girl',\n 'jardin',\n 'cherub',\n 'prada',\n 'gucci',\n 'byred',\n 'explorer',\n 'oud',\n 'tuscan',\n 'parfum',\n 'calvin',\n 'club',\n 'lutes',\n 'salt',\n 'anthology',\n \"l'air\",\n 'london',\n 'giorgio',\n 'belle',\n 'delia',\n 'ombre',\n 'christian',\n 'extract',\n 'bal',\n 'paco',\n 'encore',\n 'agents',\n 'and',\n 'cabana',\n 'molecules',\n 'davidoff',\n 'yves',\n 'desert',\n 'homme',\n 'clayton',\n 'the',\n 'tendre',\n 'frederic',\n 'saint',\n 'dior',\n 'gig',\n 'ravager',\n 'income',\n 'chanel',\n 'grand',\n 'dylan',\n \"l'imperatrice\",\n 'must',\n 'vie',\n 'rodriguez',\n 'eros',\n 'flowerbomb',\n 'bleu',\n 'sea',\n 'angel',\n 'shalimar',\n \"d'afrique\",\n 'olympia',\n 'herod',\n 'portrait',\n 'truer',\n '540',\n 'rouge',\n 'malone',\n 'bentley',\n 'afghans',\n 'drives',\n 'narco',\n 'lost',\n 'creed',\n 'fireplace',\n 'ani',\n 'viktor',\n 'kilian',\n 'orchid',\n 'noir',\n 'toy',\n 'soir',\n 'francis',\n 'trevor',\n 'habitat',\n 'eccentric',\n 'sage',\n 'ideal',\n 'hypnotic',\n 'salique',\n 'burberry',\n 'montblanc',\n 'ultra',\n 'acqua',\n 'opium',\n 'nil',\n 'men',\n 'chance',\n 'phantom',\n 'perfumes',\n 'santa',\n 'germain',\n 'terre',\n 'spicebomb',\n \"j'adore\",\n 'for',\n 'armani',\n 'carolina',\n 'grande',\n 'ford',\n 'rolf',\n 'rayanne',\n 'marocain',\n 'odour',\n 'share',\n 'marginal',\n 'promo',\n 'mademoiselle',\n 'mon',\n 'euphoria',\n 'noire',\n 'manera',\n 'paul',\n 'pour',\n 'light',\n '2018',\n 'klein',\n 'man',\n 'arian',\n 'nasomatto',\n \"l'homme\"]"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.load(open('../dataset/stopwords.json', 'r'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}